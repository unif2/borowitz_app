{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, I've decided to take out the most common words, which may not convey much meaning: stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   308  100   308    0     0   2026      0 --:--:-- --:--:-- --:--:--  2026\n"
     ]
    }
   ],
   "source": [
    "# Download the GloVe Model\n",
    "!curl -O 'http://nlp.stanford.edu/data/glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip 'glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.txt', 'r') as file:\n",
    "    first_line = file.readline()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\\n'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each line begins with the word, and continues with the components of the vector, all entered as text\n",
    "first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you only want to use the smallest GloVe vector representation\n",
    "# We form a dictionary of words. key = word, value = vector for the word embedding model\n",
    "with open('glove.6B.50d.txt', 'r') as file:\n",
    "    words = {line.split()[0]: list(float(x) for x in line.split()[1:]) for line in file}\n",
    "    file.close()\n",
    "# with open('glove_50d.pickle', 'wb') as f:\n",
    "#     pickle.dump(words, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We form a dictionary of words. key = word, value = vector for the word embedding model\n",
    "# We form 3 dictionaries corresponding to the 3 different word embeddings that we got from GloVe\n",
    "# We ignore 300D because it's pretty big\n",
    "file_names = ['glove.6B.50d.txt','glove.6B.100d.txt','glove.6B.200d.txt']\n",
    "glove_dict = {}\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'r') as file:\n",
    "        glove_dict[file_name[9:-4]] = {line.split()[0]: list(float(x) for x in line.split()[1:]) for line in file}\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store this as a pickle file\n",
    "with open('glove_dictionaries.pickle', 'wb') as f:\n",
    "    pickle.dump(glove_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('glove_dictionaries.pickle', 'rb') as f:\n",
    "#     glove_dict = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If we want to take the stop words out of the glove dictionary. But we'll just take them out of the article text.\n",
    "# for k,v in glove_dict.items():\n",
    "#     for w in stop_words:\n",
    "#         v.pop(w, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our vectorizer class. Has a transform method to transform the document into a single vector which is the\n",
    "# mean of all the word vectors.  If a word is not in the dictionary, it will be the zero vector\n",
    "class WordVectorizer50DMean(object):\n",
    "    def __init__(self, worddict):\n",
    "        self.worddict = worddict\n",
    "        self.dim = len(worddict['hello']) # length of the word vectors\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.mean([self.worddict[word] if word in self.worddict else np.zeros(self.dim) for word in X], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.38497,\n",
       " 0.80092,\n",
       " 0.064106,\n",
       " -0.28355,\n",
       " -0.026759,\n",
       " -0.34532,\n",
       " -0.64253,\n",
       " -0.11729,\n",
       " -0.33257,\n",
       " 0.55243,\n",
       " -0.087813,\n",
       " 0.9035,\n",
       " 0.47102,\n",
       " 0.56657,\n",
       " 0.6985,\n",
       " -0.35229,\n",
       " -0.86542,\n",
       " 0.90573,\n",
       " 0.03576,\n",
       " -0.071705,\n",
       " -0.12327,\n",
       " 0.54923,\n",
       " 0.47005,\n",
       " 0.35572,\n",
       " 1.2611,\n",
       " -0.67581,\n",
       " -0.94983,\n",
       " 0.68666,\n",
       " 0.3871,\n",
       " -1.3492,\n",
       " 0.63512,\n",
       " 0.46416,\n",
       " -0.48814,\n",
       " 0.83827,\n",
       " -0.9246,\n",
       " -0.33722,\n",
       " 0.53741,\n",
       " -1.0616,\n",
       " -0.081403,\n",
       " -0.67111,\n",
       " 0.30923,\n",
       " -0.3923,\n",
       " -0.55002,\n",
       " -0.68827,\n",
       " 0.58049,\n",
       " -0.11626,\n",
       " 0.013139,\n",
       " -0.57654,\n",
       " 0.048833,\n",
       " 0.67204]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of word vector. \n",
    "glove_dict['50d']['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a vectorizer called model, using our 50d dictionary\n",
    "words = glove_dict['50d']\n",
    "model = WordVectorizer50DMean(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New York Times article scraper\n",
    "# Returns article text transformed according to our model\n",
    "def get_nytimes_text(page_url):\n",
    "    page = urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    stuff = soup.find_all(\"div\", attrs={\"class\":\"css-18sbwfn StoryBodyCompanionColumn\"}) # Updated August 9, 2018\n",
    "    article_text = \"\"\n",
    "    for things in stuff:\n",
    "        blah = things.text.strip()\n",
    "        article_text += blah + ' '\n",
    "    article_text = \"\".join((char.lower() for char in article_text if char not in string.punctuation))\n",
    "    article_text = article_text.replace('\"', \" \").replace(\"'\",'').replace('“',' ').replace('”',' ').replace(\"’\",'').replace('-',' ').replace('--',' ').replace('—',' ').replace('…',' ')\n",
    "    article_text = [word for word in article_text.split() if word not in stop_words]\n",
    "    article_text = [list(model.transform(article_text))]\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onion article scraper\n",
    "# Returns article text transformed according to our model\n",
    "def get_onion_text(page_url):\n",
    "    page = urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    stuff = soup.find_all(\"p\", attrs={\"class\":None})\n",
    "    article_text = \"\"\n",
    "    for things in stuff:\n",
    "        blah = things.text.strip()\n",
    "        article_text += blah + ' '\n",
    "    article_text = \"\".join((char.lower() for char in article_text if char not in string.punctuation))\n",
    "    article_text = article_text.replace('\"', \" \").replace(\"'\",'').replace('“',' ').replace('”',' ').replace(\"’\",'').replace('-',' ').replace('--',' ').replace('—',' ').replace('…',' ')\n",
    "    article_text = [word for word in article_text.split() if word not in stop_words]\n",
    "    article_text = [list(model.transform(article_text))]\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON (derived from final_borowitz.json which was cleaned), containing all our Borowitz Report articles\n",
    "# These articles are free of stop words\n",
    "with open('final_borowitz_glove.json','r') as file:\n",
    "    borowitz = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the borowitz articles and transform them according to our model\n",
    "# At the end, we'll have a list of lists, where each individual list is a transformed article\n",
    "borowitz_articles = []\n",
    "for article in borowitz:\n",
    "    borowitz_articles.append(list(model.transform(article[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes_url = 'https://www.nytimes.com/2018/08/12/technology/google-facebook-dominance-hurts-ad-tech-firms-speeding-consolidation.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our New York Times text, transformed\n",
    "nytimes_text = get_nytimes_text(nytimes_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine everything into one matrix\n",
    "borowitz_articles.extend(nytimes_text)\n",
    "M = np.matrix(borowitz_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine_similarity function takes in a matrix M of row vectors, and computes the cosine of the angle between\n",
    "# every pair of row vectors, returning a matrix in which the i,j entry gives the cosines of the angle between\n",
    "# row vector i and row vector j. Since between every row and itself is an angle of 0, the diagonal consists of ones.\n",
    "similarity_scores = cosine_similarity(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Letter from Mark Zuckerberg | The New Yorker https://www.newyorker.com/humor/borowitz-report/a-letter-from-mark-zuckerberg Used Tool Purchased for $1.8 Million | The New Yorker https://www.newyorker.com/humor/borowitz-report/used-tool-purchased-1-8-million 931 555\n"
     ]
    }
   ],
   "source": [
    "# The last row of the matrix of scores contains the cosines of the angles between our given article (NY Times in this\n",
    "# case) and every Borowitz article. We only care about the first n-1 entries and sort them in descending order\n",
    "# We find the top two cosines, corresponding to the two Borowitz Reports that are similar to our given article.\n",
    "n = similarity_scores.shape[0]\n",
    "last_row = similarity_scores[n-1][0:n-1]\n",
    "sorted_last_row = sorted(last_row, reverse=True)\n",
    "i = np.where(last_row==sorted_last_row[0])[0][0]\n",
    "j = np.where(last_row==sorted_last_row[1])[0][0]\n",
    "print(borowitz[i][0], borowitz[i][2], borowitz[j][0], borowitz[j][2], i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
