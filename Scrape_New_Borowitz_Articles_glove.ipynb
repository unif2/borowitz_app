{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import json\n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON with past article data\n",
    "with open('final_borowitz_glove.json', 'r') as fp:\n",
    "    past_articles = json.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most recent article is the one at the top of the list.  Article link is the third entry.\n",
    "most_recent_article_url = past_articles[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_page = 'https://www.newyorker.com/humor/borowitz-report/page/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code scrapes the unique URL portion specific to each article on each page and stores them in links and\n",
    "# stops when it scrapes a link that's already in the json file\n",
    "\n",
    "def link_scraper(base_url_page):\n",
    "    print('Scraping all new article link addresses.')\n",
    "    i=1\n",
    "    links = []\n",
    "    full_link = ''\n",
    "    while full_link != most_recent_article_url:\n",
    "        sleep(0.1)\n",
    "        page = urlopen(base_url_page+str(i))\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        #stuff = soup.find_all(\"a\", attrs={\"class\":\"Link__link___3dWao  \"})\n",
    "        stuff2 = soup.find_all('li', attrs={\"class\":\"River__riverItem___3huWr\"})\n",
    "        things = []\n",
    "        for blah in stuff2:\n",
    "            if blah.find_next(\"h4\"):\n",
    "                things.append(blah.find_all(\"a\", attrs={\"class\":\"Link__link___3dWao\"}))\n",
    "\n",
    "        for thing in things:\n",
    "            link = thing[0].findNext({\"a\":\"href\"}).get('href')\n",
    "            full_link = 'https://www.newyorker.com'+link\n",
    "            if full_link == most_recent_article_url:\n",
    "                if links:\n",
    "                    print('Done. %d new links found.' %len(links))\n",
    "                else:\n",
    "                    print('Done. No new links found.')\n",
    "                return links\n",
    "            else:\n",
    "                links.append(link)\n",
    "        full_link = 'https://www.newyorker.com'+links[-1]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the article text for each article\n",
    "\n",
    "def article_clean(article):\n",
    "    article = article.replace('\\xa0', u' ')\n",
    "    article = article.replace(\"Get news satire from The Borowitz Report delivered to your inbox.\", \"\")\n",
    "    article = article.replace(\"Get the Borowitz Report delivered to your inbox.\", \"\")\n",
    "    article = article.replace(\"Get The Borowitz Report delivered to your inbox.\", \"\")\n",
    "    article = article.replace(\"(The Borowitz Report)\", \"\")\n",
    "    article = article.replace(\"News Satire from The Borowitz Report\", \"\")\n",
    "    article = article.replace(\"Satire from The Borowitz Report\", \"\")\n",
    "    article = article.replace(\"This post is news satire from The Borowitz Report.\", \"\")\n",
    "    article = article.replace(\"News satire from The Borowitz Report.\", \"\")\n",
    "    article = article.replace(\"Andy Borowitz will be doing a free show at Rutgers University on Monday, October 29, at 7 P.M. To register for tickets, click here.    Photograph by Lauren Lancaster.\", \"\")\n",
    "    article = article.replace(\"Andy Borowitz is doing a show to benefit public radio.\",\"\")\n",
    "    article = article.replace(\"Tickets for Andy Borowitz's next live show are now on sale.\", \"\")\n",
    "    article = article.replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "    article = article.replace(\"Illustration by Andy Borowitz.\", \"\")\n",
    "    article = article.replace(\"Andy Borowitz will be doing two shows at next month's New Yorker Festival: Friday, October 5th, with the storytelling group The Moth, and Saturday, October 6th, with Sarah Silverman. Ticket information here.\", \"\")\n",
    "    article = article.replace(\"A small number of tickets have just been released for Andy Borowitz's New Yorker Festival show this Friday night in New York City. Buy tickets here.\", \"\")\n",
    "    article = article.replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "    article = article.replace(\"Tickets for Andy Borowitz's next live show are now on sale.  Photograph by Alex Wong/Getty.\", \"\")\n",
    "    article = article.replace(\"Tickets for Andy Borowitz's next live show are now on sale.      Illustration by Tom Bachtell.\", \"\")\n",
    "    article = article.replace(\"Andy Borowitz will be doing two shows at next month's New Yorker Festival: Friday, October 5th, with the storytelling group The Moth, and Saturday, October 6th, with Sarah Silverman. Ticket information here.    Photograph by Tony Avelar/Bloomberg/Getty Images.\", \"\")\n",
    "    article = article.replace(\"(The Borowitz Report)\", \"\")\n",
    "    article = article.replace(\"Tickets for Andy Borowitz's next live show are now on sale.  Photograph by Chris Maddaloni/CQ Roll Call.\", \"\")\n",
    "    article = article.replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "    article = article.replace('\"', \" \").replace(\"'\",'').replace('“',' ').replace('”',' ').replace(\"’\",'').replace('-',' ').replace('--',' ').replace('—',' ').replace('…',' ')\n",
    "    article = \"\".join((char.lower() for char in article if char not in string.punctuation))\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function scrapes the article text, article title, and article date for an article\n",
    "def text_scraper(link):\n",
    "    url = 'https://www.newyorker.com'+link\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    article_text = soup.find(\"div\", attrs={\"id\":\"articleBody\"}).text\n",
    "    article_text = article_clean(article_text)\n",
    "    article_text = [x.lower() for x in article_text.split() if x.lower() not in stop_words]\n",
    "    title = soup.find('title').text\n",
    "    date = soup.find('p', attrs={'class':\"ArticleTimestamp__timestamp___1klks \"}).text\n",
    "    return [title, date, url, article_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store what text_scraper returns in d\n",
    "d = []\n",
    "\n",
    "# If a hiccup occurs (i.e., if a request was blocked), store the link that caused it in not_scraped\n",
    "not_scraped = []\n",
    "links = link_scraper(base_url_page)\n",
    "\n",
    "for link in links:\n",
    "    print('Scraping stuff from link %s' %link)\n",
    "    sleep(2)\n",
    "    try:\n",
    "        d.append(text_scraper(link))\n",
    "    except:\n",
    "        not_scraped.append(link)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If hiccups occurred, go back and try to scrape again\n",
    "if not_scraped:\n",
    "    for link in not_scraped:\n",
    "        print('Scraping stuff from link %s' %link)\n",
    "        sleep(2)\n",
    "        d.append(text_scraper(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new articles.\n"
     ]
    }
   ],
   "source": [
    "# If there are new articles, prepend the data to past_articles and write everything to json file\n",
    "if d:\n",
    "    d.extend(past_articles)\n",
    "    with open('final_borowitz_glove.json', 'w') as fp:\n",
    "        json.dump(d, fp)\n",
    "    fp.close\n",
    "else:\n",
    "    print('No new articles.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
