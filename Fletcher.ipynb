{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import urllib2\n",
    "#from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib\n",
    "from cookielib import CookieJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_borowitz():\n",
    "    file = open(\"final_borowitz.pickle\",'r')\n",
    "    d = pickle.load(file)\n",
    "\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself',\n",
    "    'she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this',\n",
    "    'that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did',\n",
    "    'doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against',\n",
    "    'between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "    'again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most',\n",
    "    'other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "    'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn',\n",
    "    'mustn','needn','shan','shouldn','wasn','weren','won','wouldn','said','mr', 'obama', 'would', 'president']\n",
    "\n",
    "    titles = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "    article_text = []\n",
    "\n",
    "    for k, v in d.items():\n",
    "        titles.append(k[0])\n",
    "        dates.append(k[1])\n",
    "        urls.append(k[2])\n",
    "        article_text.append(v)    \n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Get news satire from The Borowitz Report delivered to your inbox.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Get the Borowitz Report delivered to your inbox.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Get The Borowitz Report delivered to your inbox.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"(The Borowitz Report)\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"News Satire from The Borowitz Report\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Satire from The Borowitz Report\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"This post is news satire from The Borowitz Report.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"News satire from The Borowitz Report.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Andy Borowitz will be doing a free show at Rutgers University on Monday, October 29, at 7 P.M. To register for tickets, click here.    Photograph by Lauren Lancaster.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Andy Borowitz is doing a show to benefit public radio.\",\"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Tickets for Andy Borowitz's next live show are now on sale.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Illustration by Andy Borowitz.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Andy Borowitz will be doing two shows at next month's New Yorker Festival: Friday, October 5th, with the storytelling group The Moth, and Saturday, October 6th, with Sarah Silverman. Ticket information here.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"A small number of tickets have just been released for Andy Borowitz's New Yorker Festival show this Friday night in New York City. Buy tickets here.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Tickets for Andy Borowitz's next live show are now on sale.  Photograph by Alex Wong/Getty.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Tickets for Andy Borowitz's next live show are now on sale.      Illustration by Tom Bachtell.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Andy Borowitz will be doing two shows at next month's New Yorker Festival: Friday, October 5th, with the storytelling group The Moth, and Saturday, October 6th, with Sarah Silverman. Ticket information here.    Photograph by Tony Avelar/Bloomberg/Getty Images.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"(The Borowitz Report)\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"Tickets for Andy Borowitz's next live show are now on sale.  Photograph by Chris Maddaloni/CQ Roll Call.\", \"\")\n",
    "\n",
    "    for i in range(len(article_text)):\n",
    "        article_text[i] = article_text[i].replace(\"(Satire from The Borowitz Report)\", \"\")\n",
    "    return titles, dates, urls, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_onion_text(url):\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    #soup.prettify()\n",
    "    div_x = soup.find_all(\"div\", class_=\"content-text\")\n",
    "    return str(div_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nytimes_text(url):\n",
    "    cj = CookieJar()\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
    "    p = opener.open(url)\n",
    "\n",
    "    #print p.read()\n",
    "    page = p.read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    div_x = soup.find_all(\"p\", class_=\"story-body-text story-content\")\n",
    "    return str(div_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar_borowitz(new_text):\n",
    "    full_text = article_text + [new_text]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(full_text)\n",
    "    M = cosine_similarity(tfidf_matrix)\n",
    "    i = np.argmax(M[694][0:694])\n",
    "    return titles[i], urls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles, dates, urls, article_text = load_borowitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself',\n",
    "    'she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this',\n",
    "    'that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did',\n",
    "    'doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against',\n",
    "    'between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "    'again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most',\n",
    "    'other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "    'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn',\n",
    "    'mustn','needn','shan','shouldn','wasn','weren','won','wouldn','said','mr', 'obama', 'would', 'president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1, max_df=0.9)\n",
    "tfidf_vectorizer.fit(article_text)\n",
    "doc_vecs = tfidf_vectorizer.transform(article_text).transpose()\n",
    "#tfidf_vectorizer.get_feature_names()\n",
    "#corpus = matutils.Sparse2Corpus(doc_vecs)\n",
    "#id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.iteritems())\n",
    "#lda = models.LdaModel(corpus, id2word=id2word, num_topics=100, passes=10, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1, max_df=0.9)\n",
    "count_vectorizer.fit(article_text)\n",
    "doc_vecs2 = count_vectorizer.transform(article_text).transpose()\n",
    "#tfidf_vectorizer.get_feature_names()\n",
    "corpus2 = matutils.Sparse2Corpus(doc_vecs2)\n",
    "id2word2 = dict((v, k) for k, v in count_vectorizer.vocabulary_.iteritems())\n",
    "lda2 = models.LdaModel(corpus2, id2word=id2word2, num_topics=100, passes=10, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('yoyo.pickle', 'wb') as handle:\n",
    "#    pickle.dump([lda, corpus], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda.print_topics(num_words=20, num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = [sorted(lda.show_topic(i, topn=10), key=lambda x: x[1], reverse=True) [:10] for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"yoyo.pickle\",'r')\n",
    "yoyo_model, yoyo_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x13a9cf110>"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoyo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sorted(lda2.show_topic(4, topn=10), key=lambda x: x[1], reverse=True) [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-05d5c509ad6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#doc_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "lda_corpus = lda[corpus]\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "doc_topics = [[titles[i],lda_docs[i]] for i in range(len(article_text))]\n",
    "#doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('final_stuff.pickle', 'wb') as handle:\n",
    "    pickle.dump([lda_docs, topics], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = max(lda_docs[0], key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important = [topics[m][i][0] for i in range(len(topics[m]))]\n",
    "#[topics[m][i][0] for i in len(topics[m])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = 'Most important LDA topic composed of: %s, %s, %s, %s, %s, %s, %s, %s, %s, %s' %tuple(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important LDA topic composed of: kochs, apple, shiites, customers, sunnis, sunnis shiites, format, cook, new format, cheney\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = ['language', 'Python', 'rocks']\n",
    "some_text = \"There is a %s called %s which %s.\"\n",
    "x = some_text % tuple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a language called Python which rocks.\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fe2b288310b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_corpus2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_docs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_corpus2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc_topics2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda_docs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#doc_topics2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda2' is not defined"
     ]
    }
   ],
   "source": [
    "lda_corpus2 = lda2[corpus2]\n",
    "lda_docs2 = [doc for doc in lda_corpus2]\n",
    "doc_topics2 = [[titles[i],lda_docs2[i]] for i in range(len(article_text))]\n",
    "#doc_topics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda.print_topics(num_words=20, num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = 'dispatched the following telegram to Dewey: “I thank you for your statement which I have heard over the air a few minutes ago.”  Earlier on Election Day, Dewey told his staff that \"whatever the result, I think we have made a mighty contribution toward the unity of our country, toward the war effort and the peace to come.”  At the time, the Democratic national headquarters was in the Biltmore Hotel, on New York’s Madison Avenue.  This time, Donald J. Trump, the Republican nominee, will be watching from the New York Hilton Midtown, a few blocks from his Trump Tower home. Hillary Clinton, the Democratic nominee, will be at the Jacob K. Javits Convention Center on the West Side of Manhattan.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(analyzer='word',\n",
    "#                                   ngram_range=(1, 4), stop_words=stop_words,\n",
    "#                                   min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_nmf = TfidfVectorizer(max_df=0.95, min_df=1,\n",
    "                                   stop_words=stop_words, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_nmf = tfidf_vectorizer_nmf.fit_transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=12, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer_nmf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f080f496047d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer_nmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print_top_words(nmf, tfidf_feature_names, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer_nmf' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer_nmf.get_feature_names()\n",
    "#print_top_words(nmf, tfidf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "dtm = vectorizer.fit_transform(article_text)\n",
    "nmf_model = NMF(2)\n",
    "\n",
    "dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "#                                   ngram_range=(1, 4), stop_words=stop_words,\n",
    "#                                   min_df=1)\n",
    "\n",
    "# tfidf_vectorizer.fit(article_text)\n",
    "# doc_vecs = tfidf_vectorizer.transform(article_text).transpose()\n",
    "# corpus = matutils.Sparse2Corpus(doc_vecs)\n",
    "\n",
    "# id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda = models.LdaModel(corpus, id2word=id2word, num_topics=12, passes=10, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = tfidf_vectorizer.transform(news_text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_vecs_new = new.transpose()\n",
    "corpus_new = matutils.Sparse2Corpus(doc_vecs_new)\n",
    "\n",
    "id2word_new = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_lda = lda[corpus_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_lda_docs = [doc for doc in new_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda.update(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for doc in lda[corpus]:\n",
    "#     print(doc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_text = article_text + [news_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatched the following telegram to Dewey: “I thank you for your statement which I have heard over the air a few minutes ago.”  Earlier on Election Day, Dewey told his staff that \"whatever the result, I think we have made a mighty contribution toward the unity of our country, toward the war effort and the peace to come.”  At the time, the Democratic national headquarters was in the Biltmore Hotel, on New York’s Madison Avenue.  This time, Donald J. Trump, the Republican nominee, will be watching from the New York Hilton Midtown, a few blocks from his Trump Tower home. Hillary Clinton, the Democratic nominee, will be at the Jacob K. Javits Convention Center on the West Side of Manhattan.\n"
     ]
    }
   ],
   "source": [
    "print(full_text[694])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(M[694][0:694]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0325485816041\n"
     ]
    }
   ],
   "source": [
    "print(M[694][496])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Trump Warns Hillary May Rig Election by Getting More Votes'"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text2 = 'Edward J. Snowden, the American who has probably left the biggest mark on public policy debates during the Obama years, is today an outlaw. Mr. Snowden, a former National Security Agency contractor who disclosed to journalists secret documents detailing the United States’ mass surveillance programs, faces potential espionage charges, even though the president has acknowledged the important public debate his revelations provoked. Mr. Snowden’s whistle-blowing prompted reactions across the government. Courts found the government wrong to use Section 215 of the Patriot Act to justify mass phone data collection. Congress replaced that law with the USA Freedom Act, improving transparency about government surveillance and limiting government power to collect certain records. The president appointed an independent review board, which produced important reform recommendations. That’s just in the American government. Newspapers that published Mr. Snowden’s revelations won the Pulitzer Prize. The United Nations issued resolutions on protecting digital privacy and created a mandate to promote the right to privacy. Many technology companies, facing outrage at their apparent complicity in mass surveillance, began providing end-to-end encryption by default. Three years on, the news media still refer to Mr. Snowden and his revelations every day. His actions have brought about a dramatic increase in our awareness of the risks to our privacy in the digital age — and to the many rights that depend on privacy. Yet President Obama and the candidates to succeed him have emphasized not Mr. Snowden’s public service but the importance of prosecuting him. Hillary Clinton has said Mr. Snowden shouldn’t be brought home “without facing the music.” Donald J. Trump has said, “I think he’s a total traitor and I would deal with him harshly.” Eric H. Holder Jr. struck a more measured tone in May, about a year after he left office as Mr. Obama’s attorney general. He recognized that while Mr. Snowden broke the law, “he actually performed a public service” by raising the national debate on surveillance practices. The law the Obama administration wants to use to prosecute him takes no account of whether revealing this information was a public service. Under the antiquated Espionage Act of 1917, the only issue is whether “national defense” information was given to someone not authorized to receive it. It doesn’t matter if the secrets revealed wrongdoing or if they endangered the national defense, whether they were passed to an American journalist or to a foreign enemy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_text2 = article_text + [new_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'http://www.newyorker.com/humor/borowitz-report/snowden-drops-four-new-songs-at-sxsw'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer.fit_transform(full_text2)\n",
    "M2 = cosine_similarity(tfidf_matrix2)\n",
    "urls[np.argmax(M2[694][0:694])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text3 = 'Cringing at the mere thought of the ceremonial rite she would have to perform, Queen Elizabeth II told reporters Thursday she hopes to die before having to knight any DJs. “God willing, I’ll pass away long before I’m ever called upon to bestow an honorary knighthood on Calvin Harris or Grooverider,” said the queen, adding that she would rather be entombed in the royal burial grounds than endure a ceremony in which she grants the highest honor in the British Empire to any club DJ in recognition of their contributions to dubstep, electro house, big beat, trip-hop, dance pop, or nu-funk. “It’s only a matter of time before the requests to knight all these trance and rave DJs start pouring in. I just pray I’m a goner and worms are eating away at my decaying corpse, because there’s simply no way I’m saying ‘I dub thee Sir Jackmaster.’” The queen went on to confirm that the complete collapse of the British monarchy was far more preferable than any member of the British Royal Family having to knight Fatboy Slim.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Snowden Drops Four New Songs at SXSW'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text3 = article_text + [new_text3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Queen Offers to Restore British Rule Over United States'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1)\n",
    "tfidf_matrix3 = tfidf_vectorizer.fit_transform(full_text3)\n",
    "M3 = cosine_similarity(tfidf_matrix3)\n",
    "titles[np.argmax(M3[694][0:694])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def which_newspaper(text):\n",
    "    if text.startswith('http://www.nytimes'):\n",
    "        return get_nytimes_text(text)\n",
    "    elif text.startswith('http://www.theonion'):\n",
    "        return get_onion_text(text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_M = sorted(M[694][0:694], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(sorted_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 695)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_row = M[n-1][0:n-1]\n",
    "blah = M[n-1][0:n]\n",
    "sorted_last_row = sorted(last_row, reverse=True)\n",
    "#i, j = last_row.index(sorted_last_row[0]), last_row.index(sorted_last_row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sorted_last_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.where(last_row==sorted_last_row[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.where(blah==float(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030100124524445795"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_row[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_text = 'Cringing at the mere thought of the ceremonial rite she would have to perform, Queen Elizabeth II told reporters Thursday she hopes to die before having to knight any DJs. “God willing, I’ll pass away long before I’m ever called upon to bestow an honorary knighthood on Calvin Harris or Grooverider,” said the queen, adding that she would rather be entombed in the royal burial grounds than endure a ceremony in which she grants the highest honor in the British Empire to any club DJ in recognition of their contributions to dubstep, electro house, big beat, trip-hop, dance pop, or nu-funk. “It’s only a matter of time before the requests to knight all these trance and rave DJs start pouring in. I just pray I’m a goner and worms are eating away at my decaying corpse, because there’s simply no way I’m saying ‘I dub thee Sir Jackmaster.’” The queen went on to confirm that the complete collapse of the British monarchy was far more preferable than any member of the British Royal Family having to knight Fatboy Slim.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1919ce56e09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'arpack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_text' is not defined"
     ]
    }
   ],
   "source": [
    "full_text2 = article_text + [news_text]\n",
    "vectorizer = CountVectorizer(min_df = 1, stop_words = stop_words)\n",
    "dtm = vectorizer.fit_transform(full_text) \n",
    "pd.DataFrame(dtm.toarray(), index=range(len(full_text)), columns=vectorizer.get_feature_names()).head(10)\n",
    "lsa = TruncatedSVD(100, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "#pd.DataFrame(dtm_lsa.round(5), index = range(len(full_text)), columns = [\"component_\"+str(i) for i in range(1,101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[[i, dtm_lsa[i][0].round(5), dtm_lsa[i][1].round(5), dtm_lsa[i][2].round(5), dtm_lsa[i][3].round(5), dtm_lsa[i][4].round(5), dtm_lsa[i][5].round(5), dtm_lsa[i][6].round(5), dtm_lsa[i][7].round(5), dtm_lsa[i][8].round(5), dtm_lsa[i][9].round(5)] for i in range(len(full_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.054407</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.127381</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.079304</td>\n",
       "      <td>0.080273</td>\n",
       "      <td>0.044679</td>\n",
       "      <td>0.084182</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>0.172808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.029002</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029182</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.065013</td>\n",
       "      <td>0.473127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>0.029434</td>\n",
       "      <td>0.196643</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.126383</td>\n",
       "      <td>0.232523</td>\n",
       "      <td>0.143647</td>\n",
       "      <td>0.120874</td>\n",
       "      <td>0.178603</td>\n",
       "      <td>0.087766</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045086</td>\n",
       "      <td>0.025184</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>0.067761</td>\n",
       "      <td>0.123255</td>\n",
       "      <td>0.029182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128478</td>\n",
       "      <td>0.217874</td>\n",
       "      <td>0.126703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.262827</td>\n",
       "      <td>0.085699</td>\n",
       "      <td>0.083664</td>\n",
       "      <td>0.152583</td>\n",
       "      <td>0.367697</td>\n",
       "      <td>0.128876</td>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.216182</td>\n",
       "      <td>0.250153</td>\n",
       "      <td>0.178353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>0.096538</td>\n",
       "      <td>0.202078</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>0.116986</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.128478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282627</td>\n",
       "      <td>0.016985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.196692</td>\n",
       "      <td>0.125427</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>0.086871</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.125554</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.081392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078070</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>0.046685</td>\n",
       "      <td>0.107938</td>\n",
       "      <td>0.018203</td>\n",
       "      <td>0.065013</td>\n",
       "      <td>0.217874</td>\n",
       "      <td>0.282627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.073635</td>\n",
       "      <td>0.120988</td>\n",
       "      <td>0.137149</td>\n",
       "      <td>0.167280</td>\n",
       "      <td>0.319255</td>\n",
       "      <td>0.073393</td>\n",
       "      <td>0.200284</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>0.194826</td>\n",
       "      <td>0.381593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.174477</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.255068</td>\n",
       "      <td>0.061701</td>\n",
       "      <td>0.473127</td>\n",
       "      <td>0.126703</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 695 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "690  0.054407  0.056580  0.127381  0.178711  0.079304  0.080273  0.044679   \n",
       "691  0.029434  0.196643  0.044026  0.126383  0.232523  0.143647  0.120874   \n",
       "692  0.262827  0.085699  0.083664  0.152583  0.367697  0.128876  0.194694   \n",
       "693  0.196692  0.125427  0.024427  0.082065  0.086871  0.094853  0.046300   \n",
       "694  0.073635  0.120988  0.137149  0.167280  0.319255  0.073393  0.200284   \n",
       "\n",
       "          7         8         9      ...          685       686       687  \\\n",
       "690  0.084182  0.044891  0.172808    ...     0.013617  0.094923  0.029002   \n",
       "691  0.178603  0.087766  0.175625    ...     0.045086  0.025184  0.043477   \n",
       "692  0.216182  0.250153  0.178353    ...     0.090397  0.096538  0.202078   \n",
       "693  0.125554  0.051417  0.081392    ...     0.078070  0.032756  0.046685   \n",
       "694  0.084309  0.194826  0.381593    ...     0.110200  0.174477  0.051854   \n",
       "\n",
       "          688       689       690       691       692       693       694  \n",
       "690  0.038443  0.206758  1.000000  0.029182  0.110324  0.065013  0.473127  \n",
       "691  0.067761  0.123255  0.029182  1.000000  0.128478  0.217874  0.126703  \n",
       "692  0.011268  0.116986  0.110324  0.128478  1.000000  0.282627  0.016985  \n",
       "693  0.107938  0.018203  0.065013  0.217874  0.282627  1.000000  0.026129  \n",
       "694  0.255068  0.061701  0.473127  0.126703  0.016985  0.026129  1.000000  \n",
       "\n",
       "[5 rows x 695 columns]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T) \n",
    "pd.DataFrame(similarity.round(6),index=(range(len(full_text))), columns=(range(len(full_text))))[690:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_row = similarity[695-1][0:695-1]\n",
    "sorted_last_row = sorted(last_row, reverse=True)\n",
    "i = np.where(last_row==sorted_last_row[0])[0][0]\n",
    "j = np.where(last_row==sorted_last_row[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 392)\n"
     ]
    }
   ],
   "source": [
    "print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60525607315580709"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity[694][64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Poll: Unconscious Clinton More Fit to Be President Than Conscious Trump'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Trump: Obama Vacationing Instead of Running ISIS'"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=10, init='random', random_state=1, n_init=1, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='random', max_iter=1, n_clusters=10, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=1, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = km.predict(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reseting the labels to 3 clusters \n",
    "k_num = 10\n",
    "km = KMeans(n_clusters=k_num, random_state=1)\n",
    "km.fit(doc_vecs)\n",
    "labels = km.labels_\n",
    "\n",
    "# start by calculating the mean for each cluster\n",
    "# you can check your answer with \n",
    "km.cluster_centers_\n",
    "\n",
    "def get_cluster_centers(X, labels, k_num):\n",
    "    CC_list = []\n",
    "    for k in range(k_num):\n",
    "        # get the mean coordinates of each cluster\n",
    "        CC_list.append(np.mean(X[labels == k], axis = 0))\n",
    "    return CC_list\n",
    "\n",
    "# for each cluster substract the mean from each data point to get the error\n",
    "# then get the magnitude of each error, square it, and sum it\n",
    "def get_SSE(X, labels):\n",
    "    k_num = len(np.unique(labels))\n",
    "    CC_list = get_cluster_centers(X, labels, k_num)\n",
    "    CSEs = []\n",
    "    for k in range(k_num):\n",
    "        # for each cluster of k we get the coordinates of how far off each point is to the cluster\n",
    "        error_cords = X[labels == k] - CC_list[k]\n",
    "        # square the coordinates and sum to get the magnitude squared\n",
    "        error_cords_sq = error_cords ** 2\n",
    "        error_mag_sq = np.sum(error_cords_sq, axis = 1)\n",
    "        # since we already have the magnitude of the error squared we can just take the sum for the cluster\n",
    "        CSE = np.sum(error_mag_sq)\n",
    "        CSEs.append(CSE)\n",
    "    # sum each cluster's sum of squared errors\n",
    "    return sum(CSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# similar to the loop before\n",
    "# generate values of k, fit and label data, append Sum of Squared Errors scores to a list, and print the scores\n",
    "\n",
    "SSEs = []\n",
    "Sil_coefs = []\n",
    "for k in range(10,11):\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(doc_vecs)\n",
    "    labels = km.labels_\n",
    "    Sil_coefs.append(metrics.silhouette_score(doc_vecs, labels, metric='euclidean'))\n",
    "    SSEs.append(get_SSE(doc_vecs, labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
