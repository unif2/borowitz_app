{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_borowitz():\n",
    "    with open('final_borowitz.json', 'r') as fp:\n",
    "        d = json.load(fp)\n",
    "    fp.close()\n",
    "    titles = []\n",
    "    dates = []\n",
    "    urls = []\n",
    "    article_text = []\n",
    "\n",
    "    for article in d:\n",
    "        titles.append(article[0])\n",
    "        dates.append(article[1])\n",
    "        urls.append(article[2])\n",
    "        article_text.append(article[3])\n",
    "    return titles, dates, urls, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_onion_text(page_url):\n",
    "    page = urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    stuff = soup.find_all(\"p\", attrs={\"class\":None})\n",
    "    article_text = \"\"\n",
    "    for things in stuff:\n",
    "        blah = things.text.strip()\n",
    "        article_text += blah + ' '\n",
    "    article_text = \"\".join((char.lower() for char in article_text if char not in string.punctuation))\n",
    "    article_text = article_text.replace('\"', \" \").replace(\"'\",'').replace('“',' ').replace('”',' ').replace(\"’\",'').replace('-',' ').replace('--',' ').replace('—',' ').replace('…',' ')\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nytimes_text(page_url):\n",
    "    page = urlopen(page_url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    stuff = soup.find_all(\"div\", attrs={\"class\":\"css-18sbwfn StoryBodyCompanionColumn\"}) # Updated August 9, 2018\n",
    "    article_text = \"\"\n",
    "    for things in stuff:\n",
    "        blah = things.text.strip()\n",
    "        article_text += blah + ' '\n",
    "    article_text = \"\".join((char.lower() for char in article_text if char not in string.punctuation))\n",
    "    article_text = article_text.replace('\"', \" \").replace(\"'\",'').replace('“',' ').replace('”',' ').replace(\"’\",'').replace('-',' ').replace('--',' ').replace('—',' ').replace('…',' ')\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar_borowitz(new_text):\n",
    "    full_text = article_text + [new_text]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(full_text)\n",
    "    M = cosine_similarity(tfidf_matrix)\n",
    "    n = len(M)\n",
    "    i = np.argmax(M[n-1][0:n-1]) # [0:n-1] because we don't want the last element, because it will always be 1: new_text is\n",
    "    # obviously most similar to itself.  We don't care about that.\n",
    "    return titles[i], urls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, dates, urls, article_text = load_borowitz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself',\n",
    "    'she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this',\n",
    "    'that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did',\n",
    "    'doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against',\n",
    "    'between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "    'again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most',\n",
    "    'other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "    'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn',\n",
    "    'mustn','needn','shan','shouldn','wasn','weren','won','wouldn','said','mr', 'obama', 'would', 'president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 4), stop_words=stop_words,\n",
    "                                  min_df=1, max_df=0.9)\n",
    "tfidf_vectorizer.fit(article_text)\n",
    "doc_vecs = tfidf_vectorizer.transform(article_text).transpose()\n",
    "#tfidf_vectorizer.get_feature_names()\n",
    "corpus = matutils.Sparse2Corpus(doc_vecs)\n",
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus, id2word=id2word, num_topics=number_topics, passes=10, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_corpus = lda[corpus]\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "doc_topics = [[titles[i],lda_docs[i]] for i in range(len(article_text))]\n",
    "#doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [sorted(lda.show_topic(i, topn=10), key=lambda x: x[1], reverse=True) [:10] for i in range(number_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_stuff.pickle', 'wb') as handle:\n",
    "    pickle.dump([lda_docs, topics], handle)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"final_stuff.pickle\",'rb')\n",
    "b,c = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda.print_topics(num_words=20, num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = max(lda_docs[0], key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important = [topics[m][i][0] for i in range(len(topics[m]))]\n",
    "#[topics[m][i][0] for i in len(topics[m])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = 'Most important LDA topic composed of: %s, %s, %s, %s, %s, %s, %s, %s, %s, %s' %tuple(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important LDA topic composed of: kochs, apple, shiites, customers, sunnis, sunnis shiites, format, cook, new format, cheney\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = ['language', 'Python', 'rocks']\n",
    "some_text = \"There is a %s called %s which %s.\"\n",
    "x = some_text % tuple(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a language called Python which rocks.\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_corpus2 = lda2[corpus2]\n",
    "lda_docs2 = [doc for doc in lda_corpus2]\n",
    "doc_topics2 = [[titles[i],lda_docs2[i]] for i in range(len(article_text))]\n",
    "#doc_topics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda.print_topics(num_words=20, num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = 'dispatched the following telegram to Dewey: “I thank you for your statement which I have heard over the air a few minutes ago.”  Earlier on Election Day, Dewey told his staff that \"whatever the result, I think we have made a mighty contribution toward the unity of our country, toward the war effort and the peace to come.”  At the time, the Democratic national headquarters was in the Biltmore Hotel, on New York’s Madison Avenue.  This time, Donald J. Trump, the Republican nominee, will be watching from the New York Hilton Midtown, a few blocks from his Trump Tower home. Hillary Clinton, the Democratic nominee, will be at the Jacob K. Javits Convention Center on the West Side of Manhattan.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(analyzer='word',\n",
    "#                                   ngram_range=(1, 4), stop_words=stop_words,\n",
    "#                                   min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_nmf = TfidfVectorizer(max_df=0.95, min_df=1,\n",
    "                                   stop_words=stop_words, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_nmf = tfidf_vectorizer_nmf.fit_transform(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=12, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer_nmf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f080f496047d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer_nmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print_top_words(nmf, tfidf_feature_names, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer_nmf' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer_nmf.get_feature_names()\n",
    "#print_top_words(nmf, tfidf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "dtm = vectorizer.fit_transform(article_text)\n",
    "nmf_model = NMF(2)\n",
    "\n",
    "dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "#                                   ngram_range=(1, 4), stop_words=stop_words,\n",
    "#                                   min_df=1)\n",
    "\n",
    "# tfidf_vectorizer.fit(article_text)\n",
    "# doc_vecs = tfidf_vectorizer.transform(article_text).transpose()\n",
    "# corpus = matutils.Sparse2Corpus(doc_vecs)\n",
    "\n",
    "# id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda = models.LdaModel(corpus, id2word=id2word, num_topics=12, passes=10, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = tfidf_vectorizer.transform(news_text.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_vecs_new = new.transpose()\n",
    "corpus_new = matutils.Sparse2Corpus(doc_vecs_new)\n",
    "\n",
    "id2word_new = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_lda = lda[corpus_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_lda_docs = [doc for doc in new_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda.update(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for doc in lda[corpus]:\n",
    "#     print(doc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def which_newspaper(text):\n",
    "    if text.startswith('http://www.nytimes'):\n",
    "        return get_nytimes_text(text)\n",
    "    elif text.startswith('http://www.theonion'):\n",
    "        return get_onion_text(text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_M = sorted(M[694][0:694], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(sorted_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_row = M[n-1][0:n-1]\n",
    "blah = M[n-1][0:n]\n",
    "sorted_last_row = sorted(last_row, reverse=True)\n",
    "#i, j = last_row.index(sorted_last_row[0]), last_row.index(sorted_last_row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sorted_last_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.where(last_row==sorted_last_row[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.where(blah==float(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030100124524445795"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_row[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = 'Cringing at the mere thought of the ceremonial rite she would have to perform, Queen Elizabeth II told reporters Thursday she hopes to die before having to knight any DJs. “God willing, I’ll pass away long before I’m ever called upon to bestow an honorary knighthood on Calvin Harris or Grooverider,” said the queen, adding that she would rather be entombed in the royal burial grounds than endure a ceremony in which she grants the highest honor in the British Empire to any club DJ in recognition of their contributions to dubstep, electro house, big beat, trip-hop, dance pop, or nu-funk. “It’s only a matter of time before the requests to knight all these trance and rave DJs start pouring in. I just pray I’m a goner and worms are eating away at my decaying corpse, because there’s simply no way I’m saying ‘I dub thee Sir Jackmaster.’” The queen went on to confirm that the complete collapse of the British monarchy was far more preferable than any member of the British Royal Family having to knight Fatboy Slim.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1919ce56e09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnews_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'arpack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_text' is not defined"
     ]
    }
   ],
   "source": [
    "full_text2 = article_text + [news_text]\n",
    "vectorizer = CountVectorizer(min_df = 1, stop_words = stop_words)\n",
    "dtm = vectorizer.fit_transform(full_text) \n",
    "pd.DataFrame(dtm.toarray(), index=range(len(full_text)), columns=vectorizer.get_feature_names()).head(10)\n",
    "lsa = TruncatedSVD(100, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "#pd.DataFrame(dtm_lsa.round(5), index = range(len(full_text)), columns = [\"component_\"+str(i) for i in range(1,101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[[i, dtm_lsa[i][0].round(5), dtm_lsa[i][1].round(5), dtm_lsa[i][2].round(5), dtm_lsa[i][3].round(5), dtm_lsa[i][4].round(5), dtm_lsa[i][5].round(5), dtm_lsa[i][6].round(5), dtm_lsa[i][7].round(5), dtm_lsa[i][8].round(5), dtm_lsa[i][9].round(5)] for i in range(len(full_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "      <th>688</th>\n",
       "      <th>689</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.054407</td>\n",
       "      <td>0.056580</td>\n",
       "      <td>0.127381</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.079304</td>\n",
       "      <td>0.080273</td>\n",
       "      <td>0.044679</td>\n",
       "      <td>0.084182</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>0.172808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.029002</td>\n",
       "      <td>0.038443</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029182</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.065013</td>\n",
       "      <td>0.473127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>0.029434</td>\n",
       "      <td>0.196643</td>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.126383</td>\n",
       "      <td>0.232523</td>\n",
       "      <td>0.143647</td>\n",
       "      <td>0.120874</td>\n",
       "      <td>0.178603</td>\n",
       "      <td>0.087766</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045086</td>\n",
       "      <td>0.025184</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>0.067761</td>\n",
       "      <td>0.123255</td>\n",
       "      <td>0.029182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128478</td>\n",
       "      <td>0.217874</td>\n",
       "      <td>0.126703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.262827</td>\n",
       "      <td>0.085699</td>\n",
       "      <td>0.083664</td>\n",
       "      <td>0.152583</td>\n",
       "      <td>0.367697</td>\n",
       "      <td>0.128876</td>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.216182</td>\n",
       "      <td>0.250153</td>\n",
       "      <td>0.178353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>0.096538</td>\n",
       "      <td>0.202078</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>0.116986</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.128478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282627</td>\n",
       "      <td>0.016985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.196692</td>\n",
       "      <td>0.125427</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>0.086871</td>\n",
       "      <td>0.094853</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.125554</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.081392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078070</td>\n",
       "      <td>0.032756</td>\n",
       "      <td>0.046685</td>\n",
       "      <td>0.107938</td>\n",
       "      <td>0.018203</td>\n",
       "      <td>0.065013</td>\n",
       "      <td>0.217874</td>\n",
       "      <td>0.282627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.073635</td>\n",
       "      <td>0.120988</td>\n",
       "      <td>0.137149</td>\n",
       "      <td>0.167280</td>\n",
       "      <td>0.319255</td>\n",
       "      <td>0.073393</td>\n",
       "      <td>0.200284</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>0.194826</td>\n",
       "      <td>0.381593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.174477</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.255068</td>\n",
       "      <td>0.061701</td>\n",
       "      <td>0.473127</td>\n",
       "      <td>0.126703</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 695 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "690  0.054407  0.056580  0.127381  0.178711  0.079304  0.080273  0.044679   \n",
       "691  0.029434  0.196643  0.044026  0.126383  0.232523  0.143647  0.120874   \n",
       "692  0.262827  0.085699  0.083664  0.152583  0.367697  0.128876  0.194694   \n",
       "693  0.196692  0.125427  0.024427  0.082065  0.086871  0.094853  0.046300   \n",
       "694  0.073635  0.120988  0.137149  0.167280  0.319255  0.073393  0.200284   \n",
       "\n",
       "          7         8         9      ...          685       686       687  \\\n",
       "690  0.084182  0.044891  0.172808    ...     0.013617  0.094923  0.029002   \n",
       "691  0.178603  0.087766  0.175625    ...     0.045086  0.025184  0.043477   \n",
       "692  0.216182  0.250153  0.178353    ...     0.090397  0.096538  0.202078   \n",
       "693  0.125554  0.051417  0.081392    ...     0.078070  0.032756  0.046685   \n",
       "694  0.084309  0.194826  0.381593    ...     0.110200  0.174477  0.051854   \n",
       "\n",
       "          688       689       690       691       692       693       694  \n",
       "690  0.038443  0.206758  1.000000  0.029182  0.110324  0.065013  0.473127  \n",
       "691  0.067761  0.123255  0.029182  1.000000  0.128478  0.217874  0.126703  \n",
       "692  0.011268  0.116986  0.110324  0.128478  1.000000  0.282627  0.016985  \n",
       "693  0.107938  0.018203  0.065013  0.217874  0.282627  1.000000  0.026129  \n",
       "694  0.255068  0.061701  0.473127  0.126703  0.016985  0.026129  1.000000  \n",
       "\n",
       "[5 rows x 695 columns]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T) \n",
    "pd.DataFrame(similarity.round(6),index=(range(len(full_text))), columns=(range(len(full_text))))[690:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_row = similarity[695-1][0:695-1]\n",
    "sorted_last_row = sorted(last_row, reverse=True)\n",
    "i = np.where(last_row==sorted_last_row[0])[0][0]\n",
    "j = np.where(last_row==sorted_last_row[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 392)\n"
     ]
    }
   ],
   "source": [
    "print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60525607315580709"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity[694][64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Poll: Unconscious Clinton More Fit to Be President Than Conscious Trump'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Trump: Obama Vacationing Instead of Running ISIS'"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=10, init='random', random_state=1, n_init=1, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='random', max_iter=1, n_clusters=10, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=1, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = km.predict(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reseting the labels to 3 clusters \n",
    "k_num = 10\n",
    "km = KMeans(n_clusters=k_num, random_state=1)\n",
    "km.fit(doc_vecs)\n",
    "labels = km.labels_\n",
    "\n",
    "# start by calculating the mean for each cluster\n",
    "# you can check your answer with \n",
    "km.cluster_centers_\n",
    "\n",
    "def get_cluster_centers(X, labels, k_num):\n",
    "    CC_list = []\n",
    "    for k in range(k_num):\n",
    "        # get the mean coordinates of each cluster\n",
    "        CC_list.append(np.mean(X[labels == k], axis = 0))\n",
    "    return CC_list\n",
    "\n",
    "# for each cluster substract the mean from each data point to get the error\n",
    "# then get the magnitude of each error, square it, and sum it\n",
    "def get_SSE(X, labels):\n",
    "    k_num = len(np.unique(labels))\n",
    "    CC_list = get_cluster_centers(X, labels, k_num)\n",
    "    CSEs = []\n",
    "    for k in range(k_num):\n",
    "        # for each cluster of k we get the coordinates of how far off each point is to the cluster\n",
    "        error_cords = X[labels == k] - CC_list[k]\n",
    "        # square the coordinates and sum to get the magnitude squared\n",
    "        error_cords_sq = error_cords ** 2\n",
    "        error_mag_sq = np.sum(error_cords_sq, axis = 1)\n",
    "        # since we already have the magnitude of the error squared we can just take the sum for the cluster\n",
    "        CSE = np.sum(error_mag_sq)\n",
    "        CSEs.append(CSE)\n",
    "    # sum each cluster's sum of squared errors\n",
    "    return sum(CSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# similar to the loop before\n",
    "# generate values of k, fit and label data, append Sum of Squared Errors scores to a list, and print the scores\n",
    "\n",
    "SSEs = []\n",
    "Sil_coefs = []\n",
    "for k in range(10,11):\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(doc_vecs)\n",
    "    labels = km.labels_\n",
    "    Sil_coefs.append(metrics.silhouette_score(doc_vecs, labels, metric='euclidean'))\n",
    "    SSEs.append(get_SSE(doc_vecs, labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
